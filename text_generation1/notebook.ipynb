{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab),\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),\n",
    "    invert=True,\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list('Tensorflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f'Input: {text_from_ids(input_example).numpy()}')\n",
    "    print(f'Target: {text_from_ids(target_example).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle dataset (TF data is designed to work with possibly infinite sequences, so it doesn't attemp to shuffle the entire sequence in memory. Instead, it maintains a buffer in wich it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(f'{example_batch_predictions.shape} # (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 56, 53, 16, 38, 40, 42, 27, 33, 23, 30, 51, 36, 10, 20, 61, 22,\n",
       "       23, 16, 21, 52, 55, 52, 14, 62, 46, 28, 53, 47, 57, 25, 32, 11,  8,\n",
       "       27, 13, 26, 53, 59,  8, 50,  5, 23, 40,  7, 10, 50, 60,  3, 16,  0,\n",
       "       60,  9, 62, 58, 13, 53, 40, 47, 59, 12,  2, 46, 62, 47, 18, 51, 42,\n",
       "       46, 29,  2,  2, 52,  6, 60, 56, 13, 10, 19, 56, 14, 47, 54, 53, 22,\n",
       "        5, 46, 13, 30,  5, 52, 51, 45,  8, 59, 10, 53, 53, 29, 32],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "b\"efore, and yet you fled.\\n\\nWARWICK:\\n'Twas not your valour, Clifford, drove me thence.\\n\\nNORTHUMBERLAND\"\n",
      "Next char predictions:\n",
      "b\"JqnCYacNTJQlW3GvIJCHmpmAwgOnhrLS:-N?Mnt-k&Ja,3ku!C[UNK]u.ws?naht; gwhElcgP  m'uq?3FqAhonI&g?Q&mlf-t3nnPS\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Input:\\n{text_from_ids(input_example_batch[0]).numpy()}')\n",
    "print(f'Next char predictions:\\n{text_from_ids(sampled_indices).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss: 4.191237926483154\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(f'Prediction shape: {example_batch_predictions.shape} # (batch_size, sequence_length, vocab_size)')\n",
    "print(f'Mean loss: {example_batch_mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.10457"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'ckpts/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 10s 46ms/step - loss: 2.7329\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 9s 47ms/step - loss: 1.9934\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 1.7221\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 1.5616\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 1.4618\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.3927\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.3409\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.2961\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.2556\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.2157\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.1768\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.1366\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.0957\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.0513\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.0044\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.9560\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.9054\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.8527\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.8008\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.7510\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generate\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index\n",
    "            values=[-float('inf')] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
    "        )\n",
    "\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model. `predicted_logits.shape` is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids,\n",
    "            states=states,\n",
    "            return_state=True\n",
    "        )\n",
    "\n",
    "        # Only use the last prediction\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Thou hast I loved the moon. Now too much blood to bear\n",
      "A bloody skeet o' the loss, is chance to feed,\n",
      "As to same hither.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "O, work it will not stoop: hold your means makes the base:\n",
      "The odds for her way raise on, and love no night.\n",
      "\n",
      "ROMEO:\n",
      "Fear not, but that's my brother?\n",
      "\n",
      "ISABELLA:\n",
      "I'll soon made a consuler,\n",
      "This we have denited their festless woman?\n",
      "Here rob his lips and all extremest sheep\n",
      "To but mercy to take and beast.\n",
      "\n",
      "LUCENTIO:\n",
      "Perrain; marry, call the Volscian neighbours. Grey, come you: sir.\n",
      "\n",
      "SICINIUS:\n",
      "First, confident. What call's resolve?\n",
      "\n",
      "BRAKENBURY:\n",
      "This is stone joints, honesty only safest hit,\n",
      "Which shows me or so: could feel the soul of him.\n",
      "\n",
      "EDWARD:\n",
      "I'll bring me: one, the prove means that your most grave brave king,\n",
      "My words suffer my cheeks with skins of brains;\n",
      "Or shall we most morblit: there is many tears together\n",
      "For comfort, for would be a sad,\n",
      "to interpett at thine one and gilt thirst may spread of tears,\n",
      "Conditions and man of Pastival warms.\n",
      "Wha\n",
      "________________________________________________________________________________\n",
      "Run time: 4.203483581542969\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'{result[0].numpy().decode(\"utf-8\")}\\n{\"_\" * 80}')\n",
    "print(f'Run time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"ROMEO:\\nCall me of virtuous sea:\\nI from she such ispect light, is it not so?\\n\\nGREMIO:\\nA visil nosely in our think whereof he hath\\nMakerved this damn'd for her hand, shake thee my queen.\\n\\nLord:\\nTalten, makes the bow; perhaps with me?\\n\\nDUKE OF YORK:\\nThere's my grim and not meddle words, let it fly.\\nUndetch'd on other so.\\n\\nMARCIUS:\\nThey hate us all too soon two every; and\\nTherefore they say, with maids, lived. Do not make her weak.\\nIf ever you have left you out.\\n\\nVINCENTIO:\\nI know him now; this is so heart: but me\\nwith comfort. First, hie you to well; and go about the city;\\nIf this induction of the bring thou thou had,\\nshall, for our grmanished: if it be,\\nAnd those fines of this body stood done!\\nWe are not safe; to bear him chosen livery.\\n\\nThird Servingman:\\nWhenceford it is to intend more\\nThan to another dukes, commands. If I command,\\nAs well appeared in my truth in the book; if phessments death\\nIs made an embracherous shame: be still combath\\nThe laughter thy trong like rest,\\nAnd I am made to lo\"\n",
      " b\"ROMEO:\\nAnd I'll play the world--\\n\\nQUEEN ELIZABETH:\\nCome, cousin, I'll do't show a shame death.\\nI'll wait unon't, in meino-sadding, here shall lie done.\\n\\nHENRY BOLINGBROKE:\\nThen, good my lord; We were king should not stay\\nThese rages are death: being reason to command\\nDestroy'd his wark and all the doom of mort\\nArms in seffication so lament,\\nAnd meaning with this swords vex are no time\\nthen hurts his leisure.\\n\\nFirst Soldier:\\nFie, you do not stay\\nThe shadow of the boy, for I would not all at once more to hours life,\\nThat if deserved there and my sometim speed!\\n\\nFirst Watchman:\\nWhy, how art thou with mine eyes,--\\n\\nBUSHY:\\nMy gracious pays how to Palis.\\n\\nCORIOLANUS:\\nThis is the mid-short more!\\n\\nPAULINA:\\nWhat's your will?\\n\\nISABELLA:\\nI grant thee, skill, my cousin is in request\\nAnd with the needfellow on the business,\\nThis most upright fair designs not hereinous;\\nThe old protection of despisess so,\\nmy wisdom was descreded my bosom.\\nThis is so early to that e'er was he'er and daughters; the\\nwas more\"\n",
      " b\"ROMEO:\\nO, what may char entertainment things lost\\nUpon this island's head; and, whilst my son 'Proud's thoughts,\\nThat they be none; believe thee this, he shall as; fine,\\nAnd to take speech of calmness.\\n\\nGLOUCESTER:\\nThat last our royal hatch, before his bear: I saw him in the summon?\\n\\nSecond Servingman:\\nHere, the searchers of despised grave of me!\\n\\nKING RICHARD III:\\nYou were not stand, and good my friends: hie you.\\n\\nCORIOLANUS:\\nThou that not interes to\\nneither hear the fool way, getter you nurse.\\n\\nROMEO:\\nWell, you are setted on your garments.\\n\\nDUKE VINCENTIO:\\nLives the bride.\\n\\nFirst Soldier:\\nFaith, nothing; back.\\n\\nHORTENSIO:\\nYou may say so, she looks grow both the Tower?\\n\\nKATHARINA:\\nThinks I am a gass; sell-wings, with none of this:\\nIf their untitulash sins, and not we eat,\\nLikely in faith, I have stood the appettily of\\nme: poor sorrow will I do?\\n\\nLUCESTER:\\nWell, Lord and Latcas: your king phess held to pray\\nBy honour in my mind horse than his orach and run:\\nHere innigently, both bring fort.\\n\"\n",
      " b\"ROMEO:\\nI have known my letter than one little,\\nAnd die for me, in solem of this sword of me;\\nAnd thus I mean to stood the told me and will,\\nFor I will ask the mars of this: bestrew his life,\\nTill weep in gentle, and my master wronged\\nLies more than you deserve; fear'd away,\\nGaller by woe may go to call him health.\\n\\nCOMINIUS:\\nHa! Prave lives.\\n\\nAlloW\\nReford the templet of our Lord of York.\\nAy, close eyes, cleam till the time of justice\\nCower what is it that slew his passion moved.\\n\\nGLOUCESTER:\\nGive me thy hand, O thou wilt hear,\\nThe best of my proper Toward, hath me with his last,\\nDeften itself will slow to woman, he would not live\\nMust happieve thee that I was a poas spoke,\\nDo I wish God's sake, snave; but resolved their\\npleasant was to exter his hatred\\nThat you have seen him thank to hear.\\n\\nMONTAGUE:\\nBe cold; controll'd it: thou'rt you so tainting,\\nHis coronation--gentles, with\\nwilt thou not laugh upon this fray of work,\\nWhen eyes was dead, what's off too late, or an\\na kin, and more attendan\"\n",
      " b\"ROMEO:\\nMy Lord of Gloucester, are you so proves the duke?\\n\\nYORK:\\nEre at vex's bloody heart.\\nThe shadow will make the ladies than I will:\\nFor I have but myself so near of the Tower that shall\\nO'ly half my counsel?\\nYou have yet befeen the very near of woe\\nIs this foe chyice he there's\\ntitle, I will not perform. Their both are done, a breath,\\nthat, my wife to assistance thee.\\n\\nFirst Murderer:\\nO my liege, York not of me?\\nMethinks a devil and decay; where art thou\\nDid palterit me of I rest grawe.\\n\\nGLOUCESTER:\\nO, in much tongue gods, good Lord of Naples, worve language!\\n\\nNORTHUMBERLAND:\\nThe commonwealth be new that which my son:\\nShould the deel, when I go babe the live,\\nThat it was, something only charactes, to entering\\nAs every day is less.\\nAnd live you see, our hands with his grave.\\n\\nDUKE VINCENTIO:\\nNow that his brave before his master shall I drop\\nto Rome all mine.\\n\\nSAMPSON:\\nMelting one of the follow'd that's of sighs\\nWe may murder me in his fellows show you thus;\\nThe rangaish of our woes, who \"]\n",
      "________________________________________________________________________________\n",
      "Run time: 4.360084295272827\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'{result}\\n{\"_\" * 80}')\n",
    "print(f'Run time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000026F3E6A20A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'models')\n",
    "one_step_reloaded = tf.saved_model.load('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "I do not like the dew-crowned bagm.\n",
      "Have we have borne escaped, who lack'st their hearts may\n",
      "have y\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        inputs, labels = inputs\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.loss(labels, predictions)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 11s 45ms/step - loss: 2.7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26fbcd91ee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch: 0 | Loss: 2.1775\n",
      "Epoch: 1 | Batch: 50 | Loss: 2.0352\n",
      "Epoch: 1 | Batch: 100 | Loss: 1.9447\n",
      "Epoch: 1 | Batch: 150 | Loss: 1.8706\n",
      "Epoch: 1 | Loss: 1.9896\n",
      "Time taken for 1 epoch: 9.41 sec\n",
      "Epoch: 2 | Batch: 0 | Loss: 1.8308\n",
      "Epoch: 2 | Batch: 50 | Loss: 1.7703\n",
      "Epoch: 2 | Batch: 100 | Loss: 1.6983\n",
      "Epoch: 2 | Batch: 150 | Loss: 1.6479\n",
      "Epoch: 2 | Loss: 1.7174\n",
      "Time taken for 1 epoch: 8.56 sec\n",
      "Epoch: 3 | Batch: 0 | Loss: 1.6298\n",
      "Epoch: 3 | Batch: 50 | Loss: 1.5776\n",
      "Epoch: 3 | Batch: 100 | Loss: 1.5237\n",
      "Epoch: 3 | Batch: 150 | Loss: 1.4988\n",
      "Epoch: 3 | Loss: 1.5571\n",
      "Time taken for 1 epoch: 9.92 sec\n",
      "Epoch: 4 | Batch: 0 | Loss: 1.4679\n",
      "Epoch: 4 | Batch: 50 | Loss: 1.4857\n",
      "Epoch: 4 | Batch: 100 | Loss: 1.4549\n",
      "Epoch: 4 | Batch: 150 | Loss: 1.4414\n",
      "Epoch: 4 | Loss: 1.4582\n",
      "Time taken for 1 epoch: 9.94 sec\n",
      "Epoch: 5 | Batch: 0 | Loss: 1.4039\n",
      "Epoch: 5 | Batch: 50 | Loss: 1.3971\n",
      "Epoch: 5 | Batch: 100 | Loss: 1.3490\n",
      "Epoch: 5 | Batch: 150 | Loss: 1.3835\n",
      "Epoch: 5 | Loss: 1.3895\n",
      "Time taken for 1 epoch: 10.19 sec\n",
      "Epoch: 6 | Batch: 0 | Loss: 1.3307\n",
      "Epoch: 6 | Batch: 50 | Loss: 1.3257\n",
      "Epoch: 6 | Batch: 100 | Loss: 1.3483\n",
      "Epoch: 6 | Batch: 150 | Loss: 1.3501\n",
      "Epoch: 6 | Loss: 1.3361\n",
      "Time taken for 1 epoch: 10.01 sec\n",
      "Epoch: 7 | Batch: 0 | Loss: 1.2621\n",
      "Epoch: 7 | Batch: 50 | Loss: 1.3110\n",
      "Epoch: 7 | Batch: 100 | Loss: 1.3231\n",
      "Epoch: 7 | Batch: 150 | Loss: 1.2691\n",
      "Epoch: 7 | Loss: 1.2907\n",
      "Time taken for 1 epoch: 10.06 sec\n",
      "Epoch: 8 | Batch: 0 | Loss: 1.2266\n",
      "Epoch: 8 | Batch: 50 | Loss: 1.2348\n",
      "Epoch: 8 | Batch: 100 | Loss: 1.2538\n",
      "Epoch: 8 | Batch: 150 | Loss: 1.2809\n",
      "Epoch: 8 | Loss: 1.2489\n",
      "Time taken for 1 epoch: 10.09 sec\n",
      "Epoch: 9 | Batch: 0 | Loss: 1.1616\n",
      "Epoch: 9 | Batch: 50 | Loss: 1.1976\n",
      "Epoch: 9 | Batch: 100 | Loss: 1.1715\n",
      "Epoch: 9 | Batch: 150 | Loss: 1.2242\n",
      "Epoch: 9 | Loss: 1.2100\n",
      "Time taken for 1 epoch: 10.16 sec\n",
      "Epoch: 10 | Batch: 0 | Loss: 1.1513\n",
      "Epoch: 10 | Batch: 50 | Loss: 1.1558\n",
      "Epoch: 10 | Batch: 100 | Loss: 1.1763\n",
      "Epoch: 10 | Batch: 150 | Loss: 1.1700\n",
      "Epoch: 10 | Loss: 1.1699\n",
      "Time taken for 1 epoch: 10.29 sec\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f'Epoch: {epoch + 1} | Batch: {batch_n} | Loss: {logs[\"loss\"]:.4f}'\n",
    "            print(template)\n",
    "\n",
    "    # Saving the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} sec')\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78e99f3709755862256ae760bbb467c2505ba6263dcaf1a57b6767330b45fd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
